{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9b718e",
   "metadata": {},
   "source": [
    "# AC Time Series JEPA - Interactive Explore\n",
    "\n",
    "Explore data, run training steps, and verify training works. Run cells in Jupyter or VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449c096",
   "metadata": {
    "title": "imports and setup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12fda7",
   "metadata": {
    "title": "load data"
   },
   "outputs": [],
   "source": [
    "from eb_jepa.datasets.utils import init_data\n",
    "\n",
    "loader, val_loader, data_config = init_data(\"hft_timeseries\")\n",
    "x, a, loc, _, _ = next(iter(loader))\n",
    "\n",
    "print(\"Batch shapes: x\", x.shape, \"| a\", a.shape, \"| loc\", loc.shape)\n",
    "print(\"Batches:\", len(loader), \"| batch_size:\", data_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eebddc",
   "metadata": {
    "title": "plot sample sequence"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 6), sharex=True)\n",
    "sample_idx = 0\n",
    "t = torch.arange(loc.shape[1])\n",
    "axes[0].plot(t, loc[sample_idx, :, 0].numpy(), label=\"dPrice\")\n",
    "axes[0].set_ylabel(\"dPrice\")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "axes[1].plot(t, loc[sample_idx, :, 1].numpy(), label=\"volume\", color=\"green\")\n",
    "axes[1].set_ylabel(\"Volume\")\n",
    "axes[1].legend(loc=\"upper right\")\n",
    "axes[2].plot(t, loc[sample_idx, :, 2].numpy(), label=\"spread\", color=\"orange\")\n",
    "axes[2].set_ylabel(\"Spread\")\n",
    "axes[2].set_xlabel(\"Time step\")\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "plt.suptitle(\"Sample sequence: state (dPrice, volume, spread)\")\n",
    "plt.tight_layout()\n",
    "out_dir = ROOT / \"notebooks\" / \"outputs\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "plt.savefig(out_dir / \"sample_sequence.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ddfe36",
   "metadata": {
    "title": "build model"
   },
   "outputs": [],
   "source": [
    "from eb_jepa.architectures import (\n",
    "    InverseDynamicsModel,\n",
    "    RNNPredictor,\n",
    "    TimeSeriesEncoder,\n",
    ")\n",
    "from eb_jepa.jepa import JEPA, JEPAProbe\n",
    "from eb_jepa.losses import SquareLossSeq, VC_IDM_Sim_Regularizer\n",
    "from eb_jepa.state_decoder import MLPStateHead\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim, action_dim = 3, 1\n",
    "seq_len = data_config.seq_len\n",
    "nsteps = 8\n",
    "\n",
    "encoder = TimeSeriesEncoder(input_dim=state_dim, hidden_dim=64, output_dim=256)\n",
    "test_out = encoder(torch.rand(1, state_dim, seq_len, 1, 1))\n",
    "_, f, _, h, w = test_out.shape\n",
    "\n",
    "predictor = RNNPredictor(\n",
    "    hidden_size=encoder.mlp_output_dim,\n",
    "    action_dim=action_dim,\n",
    "    final_ln=nn.LayerNorm(encoder.mlp_output_dim),\n",
    ")\n",
    "idm = InverseDynamicsModel(\n",
    "    state_dim=h * w * f,\n",
    "    hidden_dim=256,\n",
    "    action_dim=action_dim,\n",
    ").to(device)\n",
    "regularizer = VC_IDM_Sim_Regularizer(\n",
    "    cov_coeff=8, std_coeff=16, sim_coeff_t=12, idm_coeff=1,\n",
    "    idm=idm, first_t_only=False, projector=None,\n",
    "    spatial_as_samples=False, idm_after_proj=False, sim_t_after_proj=False,\n",
    ")\n",
    "jepa = JEPA(encoder, nn.Identity(), predictor, regularizer, SquareLossSeq()).to(device)\n",
    "\n",
    "ds = loader.dataset.dataset if hasattr(loader.dataset, \"dataset\") else loader.dataset\n",
    "state_head = MLPStateHead(\n",
    "    input_dim=encoder.mlp_output_dim,\n",
    "    output_dim=state_dim,\n",
    "    normalizer=getattr(ds, \"normalizer\", None),\n",
    ").to(device)\n",
    "state_prober = JEPAProbe(jepa=jepa, head=state_head, hcost=nn.MSELoss())\n",
    "\n",
    "jepa_opt = AdamW(jepa.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "probe_opt = AdamW(state_head.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scaler = GradScaler(device.type, enabled=True)\n",
    "dtype = torch.float16\n",
    "\n",
    "print(\"Model built. Encoder out dim:\", encoder.mlp_output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b0512",
   "metadata": {
    "title": "training loop - run N steps and record losses"
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 50\n",
    "losses = []\n",
    "\n",
    "for step, (x_b, a_b, loc_b, _, _) in enumerate(loader):\n",
    "    if step >= NUM_STEPS:\n",
    "        break\n",
    "    x_b = x_b.permute(0, 2, 1, 3, 4).to(device)\n",
    "    a_b = a_b.permute(0, 2, 1).to(device)\n",
    "    loc_b = loc_b.permute(0, 2, 1).to(device)\n",
    "\n",
    "    jepa_opt.zero_grad()\n",
    "    with autocast(device.type, enabled=True, dtype=dtype):\n",
    "        _, (jepa_loss, regl, _, _, pl) = jepa.unroll(\n",
    "            x_b, a_b, nsteps=nsteps,\n",
    "            unroll_mode=\"autoregressive\", ctxt_window_time=1,\n",
    "            compute_loss=True, return_all_steps=False,\n",
    "        )\n",
    "    scaler.scale(jepa_loss).backward()\n",
    "    scaler.step(jepa_opt)\n",
    "    scaler.update()\n",
    "\n",
    "    probe_opt.zero_grad()\n",
    "    with autocast(device.type, enabled=True, dtype=dtype):\n",
    "        probe_loss = state_prober(\n",
    "            observations=x_b[:, :, :1],\n",
    "            targets=loc_b[:, :, :1],\n",
    "        )\n",
    "    scaler.scale(probe_loss).backward()\n",
    "    scaler.step(probe_opt)\n",
    "    scaler.update()\n",
    "\n",
    "    total = jepa_loss.item() + probe_loss.item()\n",
    "    losses.append({\"total\": total, \"jepa\": jepa_loss.item(), \"pred\": pl.item(), \"probe\": probe_loss.item()})\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:3d} | loss={total:.4f} | pred={pl.item():.4f} | probe={probe_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb59c3",
   "metadata": {
    "title": "plot training loss"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot([l[\"total\"] for l in losses], label=\"total\")\n",
    "ax.plot([l[\"pred\"] for l in losses], label=\"pred\", alpha=0.8)\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Training loss over steps\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"training_loss.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f63bb",
   "metadata": {
    "title": "test: verify training goes well (loss should decrease)"
   },
   "outputs": [],
   "source": [
    "first_avg = sum(l[\"total\"] for l in losses[:10]) / min(10, len(losses))\n",
    "last_avg = sum(l[\"total\"] for l in losses[-10:]) / min(10, len(losses))\n",
    "decreased = last_avg < first_avg\n",
    "print(f\"First 10 steps avg loss: {first_avg:.4f}\")\n",
    "print(f\"Last 10 steps avg loss:  {last_avg:.4f}\")\n",
    "print(f\"Loss decreased: {decreased} {'✓' if decreased else '✗'}\")\n",
    "assert decreased, \"Training failed: loss did not decrease\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdca28b",
   "metadata": {
    "title": "inference"
   },
   "outputs": [],
   "source": [
    "x_infer, _, _, _, _ = next(iter(loader))\n",
    "x_infer = x_infer.permute(0, 2, 1, 3, 4).to(device)\n",
    "with torch.no_grad():\n",
    "    z = jepa.encode(x_infer)\n",
    "print(\"Encoded shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3486b4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Data**: HFT state (dPrice, volume, spread) + action (imbalance)\n",
    "- **Model**: TimeSeriesEncoder + RNNPredictor + VC_IDM_Sim_Regularizer\n",
    "- **Full train**: `uv run python -m examples.ac_timeseries_jepa.main`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
